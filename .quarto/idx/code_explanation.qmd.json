{"title":"Scraping Over 2000 PDF Pages!","markdown":{"yaml":{"title":"Scraping Over 2000 PDF Pages!","author":"Havisha Khurana","date":"today","date-format":"full","editor_options":{"chunk_output_type":"console"},"format":{"closeread-html":{"embed-resources":true,"theme":"journal","highlight":"tango","df-print":"paged","code-overflow":"wrap","code-line-numbers":true,"code-copy":true,"highlight-style":"pygments"}},"execute":{"warning":false,"error":false,"message":false,"echo":true,"eval":false}},"headingText":"grab each entry-level information","containsRefs":false,"markdown":"\n\n<br>\n\nOne of the data sources I worked on for the project were public documentation from the Oregon Department of Education (ODE) that detailed state funds allocation to school districts by student types. This document came in the form of structured .pdfs. The primary 'Data Sciency' task I undertook during the capstone was to scrape 14 pdfs of 233 pages each.\n\n<br>\n\nIn this section, I explain the logic of the text-scraping code. All these functions are found in the **code/** folder. <br>\n\nLet's start by looking at one pdf page.\n\n![](closeread_figures/pdf_scrape_eg.png){fig-align=\"center\" width=\"800\"}\n\n<br>\n\n::::::: cr-section\n::: {#cr-readpdf}\n```{r}\n#| echo: true\n#| eval: false\nscrape_pdf_data <- function(file_path) {\n\n  file_text_lines <- pdf_text(file_path) %>%\n    map2_dfr(., 1:length(.), ~ {\n      text <- strsplit(.x, \"\\n\")[[1]]\n      text <- text[!grepl(\"^\\\\s*$\", text)]\n      text <- gsub(\"^\\\\s\\\\s+\", \"\", text)\n      text <- gsub(\"\\\\s+\", \" \", text)\n      text <- gsub(\"‐\", \"-\", text)\n\n      page_df <- data.frame(text = text, page = .y)\n\n      return(page_df)\n    })\n\n  [...]\n}\n```\n:::\n\nThe first step is the read the pdf as a dataframe. @cr-readpdf\n\nAs parameter, I pass the relative file path where the pdf is saved. [@cr-readpdf]{highlight=\"1\"}\n\nI use the **pdf_text()** function from the *pdftools* package which converts each pdf page into an object of type string. [@cr-readpdf]{highlight=\"3\"}\n\nThen, I apply a series of cleaning process across each page-character object using the **map2_dfr()** function from the *purr* package. [@cr-readpdf]{highlight=\"4\"}\n\nThe cleaning steps include breaking the string into different lines separating at **next line** delimiter, removing white spaces, and multiple spaces in each line, and changing the hyphen symbol. [@cr-readpdf]{highlight=\"5-9\"}\n\nNow each line of each page is a different character object. I collapse all of them into a single dataframe. [@cr-readpdf]{highlight=\"11\"}\n\n::: {#cr-flaglines}\n```{r}\n#| echo: true\n#| eval: false\nscrape_pdf_data <- function(file_path) {\n  [...]\n  \n  get_district_lines <- file_text_lines %>%\n    mutate(district_id_found = as.integer(grepl(\"District ID:\", text)))\n    select(page, district_id_found) %>%\n    distinct() %>%\n    filter(district_id_found == 1) %>%\n    mutate(district_index = row_number()) %>%\n    select(-district_id_found) %>%\n    full_join(tibble(page = 1:max(file_text_lines$page)), by = \"page\") %>%\n    arrange(page) %>%\n    fill(district_index, .direction = \"down\")\n\n  district_year_list <- file_text_lines %>%\n    left_join(get_district_lines) %>%\n    split(., .$district_index)\n\n  return(district_year_list)\n}\n```\n:::\n\nIn the next function, I find lines corresponding to district enteries so that I can leverage the structure. @cr-flaglines\n\nThen, I split the dataframe by district, so that each district has all its lines in a separate object. [@cr-flaglines]{highlight=\"15-17\"}\n\n::: {#cr-regularexpression}\n```{r}\n#| echo: true\n#| eval: false\none_district_info <- function(district_df) {\n  district_df <- district_df %>%\n    mutate(\n      information_type = case_when(\n        grepl(\"District ID\", text) ~ \"district_info\",\n        # adding as files from 2014 onwards have this in the entry name\n        grepl(\"for funding calculation|information only\", text) ~ \"entry_name\",\n        grepl(\":\", text) ~ \"category_info\",\n        grepl(\"ADMw\", text) ~ \"total\",\n        TRUE ~ \"entry_name\"\n      )\n    )  %>%\n    mutate(entry_index = cumsum(information_type == \"entry_name\"))\n  \n  # grab district level info\n  district_details <- district_df %>%\n    filter(information_type == \"district_info\") %>%\n    select(text) %>%\n    unlist() %>%\n    str_match(., \"^(.*?),\\\\s*(.*?)\\\\s*District ID:\\\\s*(\\\\d+)$\")\n   \n  [...]\n```\n:::\n\nThis function shows the steps to extract information from lines related to a single district. @cr-regularexpression\n\nThe function parameters is a district-specific dataframe, created above. [@cr-regularexpression]{highlight=\"1\"}\n\nI classify all the lines in accordance with the structure. [@cr-regularexpression]{highlight=\"4-11\"}\n\nThen, I use regular expressions to extract key details based on *information_type* [@cr-regularexpression]{highlight=\"17,20\"}\n\n::: {#cr-regularexpression2}\n```{r}\n#| echo: true\n#| eval: false\n\none_district_info <- function(district_df) {\n  [...]\n  entry_category_information <- district_df %>%\n    filter(entry_index != 0) %>%\n    split(., .$entry_index) %>%\n    map_dfr(., ~ {\n      # pattern different for 2024, no = sign\n      category_pattern <-  \"(.*):\\\\s*(-?[\\\\d,]+\\\\.\\\\d+)?\\\\s*X\\\\s*(-?[\\\\d.]+)\n      \\\\s*=?\\\\s*(-?[\\\\d,]+\\\\.\\\\d+)?\\\\s*(-?[\\\\d,]+\\\\.\\\\d+)?\\\\s*X\\\\s*(-?[\\\\d.]+)\n      \\\\s*=?\\\\s*(-?[\\\\d,]+\\\\.\\\\d+)?\"\n      \n      \n      .x %>%\n        filter(information_type == \"category_info\") %>%\n        select(text) %>%\n        unlist() %>%\n        str_match(., category_pattern) %>%\n        as.data.frame() %>%\n        select(-V7) %>%\n        rename(\n          \"org_text\" = \"V1\",\n          \"category\" = \"V2\",\n          \"current_year_adm\" = \"V3\",\n          \"category_weight\" = \"V4\",\n          \"current_year_admw\" = \"V5\",\n          \"past_year_adm\" = \"V6\",\n          \"past_year_admw\" = \"V8\"\n        ) \n    )\n  \n  [...]\n}\n```\n:::\n\nAnother instance where I used a regular expressions to extract key details [@cr-regularexpression2]{highlight=\"9-11\"}\n\nLastly, I organize this information as a dataframe. [@cr-regularexpression2]{highlight=\"7\"}\n\nLastly, I organize this information as a dataframe. [@cr-regularexpression2]{highlight=\"19-29\"}\n:::::::\n\n<br>\n\nI go through this process for all pdfs. Now, Let's walk-through the steps using an example.\n\n<br><br>\n","srcMarkdownNoYaml":"\n\n<br>\n\nOne of the data sources I worked on for the project were public documentation from the Oregon Department of Education (ODE) that detailed state funds allocation to school districts by student types. This document came in the form of structured .pdfs. The primary 'Data Sciency' task I undertook during the capstone was to scrape 14 pdfs of 233 pages each.\n\n<br>\n\nIn this section, I explain the logic of the text-scraping code. All these functions are found in the **code/** folder. <br>\n\nLet's start by looking at one pdf page.\n\n![](closeread_figures/pdf_scrape_eg.png){fig-align=\"center\" width=\"800\"}\n\n<br>\n\n::::::: cr-section\n::: {#cr-readpdf}\n```{r}\n#| echo: true\n#| eval: false\nscrape_pdf_data <- function(file_path) {\n\n  file_text_lines <- pdf_text(file_path) %>%\n    map2_dfr(., 1:length(.), ~ {\n      text <- strsplit(.x, \"\\n\")[[1]]\n      text <- text[!grepl(\"^\\\\s*$\", text)]\n      text <- gsub(\"^\\\\s\\\\s+\", \"\", text)\n      text <- gsub(\"\\\\s+\", \" \", text)\n      text <- gsub(\"‐\", \"-\", text)\n\n      page_df <- data.frame(text = text, page = .y)\n\n      return(page_df)\n    })\n\n  [...]\n}\n```\n:::\n\nThe first step is the read the pdf as a dataframe. @cr-readpdf\n\nAs parameter, I pass the relative file path where the pdf is saved. [@cr-readpdf]{highlight=\"1\"}\n\nI use the **pdf_text()** function from the *pdftools* package which converts each pdf page into an object of type string. [@cr-readpdf]{highlight=\"3\"}\n\nThen, I apply a series of cleaning process across each page-character object using the **map2_dfr()** function from the *purr* package. [@cr-readpdf]{highlight=\"4\"}\n\nThe cleaning steps include breaking the string into different lines separating at **next line** delimiter, removing white spaces, and multiple spaces in each line, and changing the hyphen symbol. [@cr-readpdf]{highlight=\"5-9\"}\n\nNow each line of each page is a different character object. I collapse all of them into a single dataframe. [@cr-readpdf]{highlight=\"11\"}\n\n::: {#cr-flaglines}\n```{r}\n#| echo: true\n#| eval: false\nscrape_pdf_data <- function(file_path) {\n  [...]\n  \n  get_district_lines <- file_text_lines %>%\n    mutate(district_id_found = as.integer(grepl(\"District ID:\", text)))\n    select(page, district_id_found) %>%\n    distinct() %>%\n    filter(district_id_found == 1) %>%\n    mutate(district_index = row_number()) %>%\n    select(-district_id_found) %>%\n    full_join(tibble(page = 1:max(file_text_lines$page)), by = \"page\") %>%\n    arrange(page) %>%\n    fill(district_index, .direction = \"down\")\n\n  district_year_list <- file_text_lines %>%\n    left_join(get_district_lines) %>%\n    split(., .$district_index)\n\n  return(district_year_list)\n}\n```\n:::\n\nIn the next function, I find lines corresponding to district enteries so that I can leverage the structure. @cr-flaglines\n\nThen, I split the dataframe by district, so that each district has all its lines in a separate object. [@cr-flaglines]{highlight=\"15-17\"}\n\n::: {#cr-regularexpression}\n```{r}\n#| echo: true\n#| eval: false\none_district_info <- function(district_df) {\n  district_df <- district_df %>%\n    mutate(\n      information_type = case_when(\n        grepl(\"District ID\", text) ~ \"district_info\",\n        # adding as files from 2014 onwards have this in the entry name\n        grepl(\"for funding calculation|information only\", text) ~ \"entry_name\",\n        grepl(\":\", text) ~ \"category_info\",\n        grepl(\"ADMw\", text) ~ \"total\",\n        TRUE ~ \"entry_name\"\n      )\n    )  %>%\n    mutate(entry_index = cumsum(information_type == \"entry_name\"))\n  \n  # grab district level info\n  district_details <- district_df %>%\n    filter(information_type == \"district_info\") %>%\n    select(text) %>%\n    unlist() %>%\n    str_match(., \"^(.*?),\\\\s*(.*?)\\\\s*District ID:\\\\s*(\\\\d+)$\")\n   \n  [...]\n```\n:::\n\nThis function shows the steps to extract information from lines related to a single district. @cr-regularexpression\n\nThe function parameters is a district-specific dataframe, created above. [@cr-regularexpression]{highlight=\"1\"}\n\nI classify all the lines in accordance with the structure. [@cr-regularexpression]{highlight=\"4-11\"}\n\nThen, I use regular expressions to extract key details based on *information_type* [@cr-regularexpression]{highlight=\"17,20\"}\n\n::: {#cr-regularexpression2}\n```{r}\n#| echo: true\n#| eval: false\n\n# grab each entry-level information\none_district_info <- function(district_df) {\n  [...]\n  entry_category_information <- district_df %>%\n    filter(entry_index != 0) %>%\n    split(., .$entry_index) %>%\n    map_dfr(., ~ {\n      # pattern different for 2024, no = sign\n      category_pattern <-  \"(.*):\\\\s*(-?[\\\\d,]+\\\\.\\\\d+)?\\\\s*X\\\\s*(-?[\\\\d.]+)\n      \\\\s*=?\\\\s*(-?[\\\\d,]+\\\\.\\\\d+)?\\\\s*(-?[\\\\d,]+\\\\.\\\\d+)?\\\\s*X\\\\s*(-?[\\\\d.]+)\n      \\\\s*=?\\\\s*(-?[\\\\d,]+\\\\.\\\\d+)?\"\n      \n      \n      .x %>%\n        filter(information_type == \"category_info\") %>%\n        select(text) %>%\n        unlist() %>%\n        str_match(., category_pattern) %>%\n        as.data.frame() %>%\n        select(-V7) %>%\n        rename(\n          \"org_text\" = \"V1\",\n          \"category\" = \"V2\",\n          \"current_year_adm\" = \"V3\",\n          \"category_weight\" = \"V4\",\n          \"current_year_admw\" = \"V5\",\n          \"past_year_adm\" = \"V6\",\n          \"past_year_admw\" = \"V8\"\n        ) \n    )\n  \n  [...]\n}\n```\n:::\n\nAnother instance where I used a regular expressions to extract key details [@cr-regularexpression2]{highlight=\"9-11\"}\n\nLastly, I organize this information as a dataframe. [@cr-regularexpression2]{highlight=\"7\"}\n\nLastly, I organize this information as a dataframe. [@cr-regularexpression2]{highlight=\"19-29\"}\n:::::::\n\n<br>\n\nI go through this process for all pdfs. Now, Let's walk-through the steps using an example.\n\n<br><br>\n"},"formats":{"closeread-html":{"identifier":{"display-name":"HTML","target-format":"closeread-html","base-format":"html","extension-name":"closeread"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":false,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"shortcodes":["C:\\Users\\havis\\University of Oregon Dropbox\\Havi Khurana\\Courses\\2024_02_Winter\\EDLD DS Capstone\\sped-oregon-finance\\_extensions\\qmd-lab\\closeread\\spacer.lua"]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["C:\\Users\\havis\\University of Oregon Dropbox\\Havi Khurana\\Courses\\2024_02_Winter\\EDLD DS Capstone\\sped-oregon-finance\\_extensions\\qmd-lab\\closeread\\closeread.lua"],"css":["_extensions/qmd-lab/closeread/closeread.css"],"embed-resources":true,"highlight-style":"pygments","output-file":"code_explanation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","page-layout":"full","revealjs-plugins":[],"title":"Scraping Over 2000 PDF Pages!","author":"Havisha Khurana","date":"today","date-format":"full","editor_options":{"chunk_output_type":"console"},"theme":"journal","highlight":"tango","code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}