{
  "hash": "7010586c48f2841fdecdab37c2580dda",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Scraping Over 2000 PDF Pages!\nauthor: \"Havisha Khurana\"\ndate: \"today\"\ndate-format: \"full\"\neditor_options: \n  chunk_output_type: console\nformat: \n  closeread-html:\n     embed-resources: true\n     theme: journal\n     highlight: tango\n     df-print: paged\n     code-overflow: wrap\n     code-line-numbers: true\n     code-copy: true\n     highlight-style: pygments\nexecute:\n   warning: false\n   error: false\n   message: false\n   echo: true\n   eval: false\n---\n\n\n\n<br>\n\nOne of the data sources I worked on for the project were public documentation from the Oregon Department of Education (ODE) that detailed state funds allocation to school districts by student types. This document came in the form of structured .pdfs. The primary 'Data Sciency' task I undertook during the capstone was to scrape 14 pdfs of 233 pages each.\n\n<br>\n\nIn this section, I explain the logic of the text-scraping code. All these functions are found in the **code/** folder. <br>\n\nLet's start by look at one pdf page.\n\n![](closeread_figures/pdf_scrape_eg.png){fig-align=\"center\" width=\"800\"}\n\n<br>\n\n::::::: cr-section\n::: {#cr-readpdf}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscrape_pdf_data <- function(file_path) {\n\n  file_text_lines <- pdf_text(file_path) %>%\n    map2_dfr(., 1:length(.), ~ {\n      text <- strsplit(.x, \"\\n\")[[1]]\n      text <- text[!grepl(\"^\\\\s*$\", text)]\n      text <- gsub(\"^\\\\s\\\\s+\", \"\", text)\n      text <- gsub(\"\\\\s+\", \" \", text)\n      text <- gsub(\"â€\", \"-\", text)\n\n      page_df <- data.frame(text = text, page = .y)\n\n      return(page_df)\n    })\n\n  [...]\n}\n```\n:::\n\n\n:::\n\nThe first step is the read the pdf as a dataframe. @cr-readpdf\n\nAs parameter, I pass the relative file path where the pdf is saved. [@cr-readpdf]{highlight=\"1\"}\n\nI use the **pdf_text()** function from the *pdftools* package which converts each pdf page into an object of type string. [@cr-readpdf]{highlight=\"3\"}\n\nThen, I apply a series of cleaning process across each page-character object using the **map2_dfr()** function from the *purr* package. [@cr-readpdf]{highlight=\"4\"}\n\nThe cleaning steps include breaking the string into different lines separating at **next line** delimiter, removing white spaces, and multiple spaces in each line, and changing the hyphen symbol. [@cr-readpdf]{highlight=\"5-9\"}\n\nNow each line of each page is a different character object. I collapse all of them into a single dataframe. [@cr-readpdf]{highlight=\"11\"}\n\n::: {#cr-flaglines}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscrape_pdf_data <- function(file_path) {\n  [...]\n  \n  get_district_lines <- file_text_lines %>%\n    mutate(district_id_found = as.integer(grepl(\"District ID:\", text)))\n    select(page, district_id_found) %>%\n    distinct() %>%\n    filter(district_id_found == 1) %>%\n    mutate(district_index = row_number()) %>%\n    select(-district_id_found) %>%\n    full_join(tibble(page = 1:max(file_text_lines$page)), by = \"page\") %>%\n    arrange(page) %>%\n    fill(district_index, .direction = \"down\")\n\n  district_year_list <- file_text_lines %>%\n    left_join(get_district_lines) %>%\n    split(., .$district_index)\n\n  return(district_year_list)\n}\n```\n:::\n\n\n:::\n\nIn the next function, I find lines corresponding to district enteries so that I can leverage the structure. @cr-flaglines\n\nThen, I split the dataframe by district, so that each district has all its lines in a separate object. [@cr-flaglines]{highlight=\"15-17\"}\n\n::: {#cr-regularexpression}\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_district_info <- function(district_df) {\n  district_df <- district_df %>%\n    mutate(\n      information_type = case_when(\n        grepl(\"District ID\", text) ~ \"district_info\",\n        # adding as files from 2014 onwards have this in the entry name\n        grepl(\"for funding calculation|information only\", text) ~ \"entry_name\",\n        grepl(\":\", text) ~ \"category_info\",\n        grepl(\"ADMw\", text) ~ \"total\",\n        TRUE ~ \"entry_name\"\n      )\n    )  %>%\n    mutate(entry_index = cumsum(information_type == \"entry_name\"))\n  \n  # grab district level info\n  district_details <- district_df %>%\n    filter(information_type == \"district_info\") %>%\n    select(text) %>%\n    unlist() %>%\n    str_match(., \"^(.*?),\\\\s*(.*?)\\\\s*District ID:\\\\s*(\\\\d+)$\")\n   \n  [...]\n```\n:::\n\n\n:::\n\nThis function shows the steps to extract information from lines related to a single district. @cr-regularexpression\n\nThe function parameters is a district-specific dataframe, created above. [@cr-regularexpression]{highlight=\"1\"}\n\nI classify all the lines in accordance with the structure. [@cr-regularexpression]{highlight=\"4-11\"}\n\nThen, I use regular expressions to extract key details based on *information_type* [@cr-regularexpression]{highlight=\"17,20\"}\n\n::: {#cr-regularexpression2}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# grab each entry-level information\none_district_info <- function(district_df) {\n  [...]\n  entry_category_information <- district_df %>%\n    filter(entry_index != 0) %>%\n    split(., .$entry_index) %>%\n    map_dfr(., ~ {\n      # pattern different for 2024, no = sign\n      category_pattern <-  \"(.*):\\\\s*(-?[\\\\d,]+\\\\.\\\\d+)?\\\\s*X\\\\s*(-?[\\\\d.]+)\n      \\\\s*=?\\\\s*(-?[\\\\d,]+\\\\.\\\\d+)?\\\\s*(-?[\\\\d,]+\\\\.\\\\d+)?\\\\s*X\\\\s*(-?[\\\\d.]+)\n      \\\\s*=?\\\\s*(-?[\\\\d,]+\\\\.\\\\d+)?\"\n      \n      \n      .x %>%\n        filter(information_type == \"category_info\") %>%\n        select(text) %>%\n        unlist() %>%\n        str_match(., category_pattern) %>%\n        as.data.frame() %>%\n        select(-V7) %>%\n        rename(\n          \"org_text\" = \"V1\",\n          \"category\" = \"V2\",\n          \"current_year_adm\" = \"V3\",\n          \"category_weight\" = \"V4\",\n          \"current_year_admw\" = \"V5\",\n          \"past_year_adm\" = \"V6\",\n          \"past_year_admw\" = \"V8\"\n        ) \n    )\n  \n  [...]\n}\n```\n:::\n\n\n:::\n\nAnother instance where I used a regular expressions to extract key details [@cr-regularexpression2]{highlight=\"9-11\"}\n\nLastly, I organize this information as a dataframe. [@cr-regularexpression2]{highlight=\"7\"}\n\nLastly, I organize this information as a dataframe. [@cr-regularexpression2]{highlight=\"19-29\"}\n:::::::\n\n<br>\n\nI go through this process for all pdfs. Now, Let's walk-through the steps using an example.\n\n<br><br>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}